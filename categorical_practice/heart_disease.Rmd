---
title: "R Notebook"
---

#Strategy:
1. load libraries
2. load data
3. EDA
4. Cleaning
5. Feature Engineering
6. Training/Testing Sets
7. Model selection and building iteration
8. Model Comparison
9. Visualizations
10. Results/Next Steps

#1. load libraries
```{r}
#load libraries
pacman::p_load(funModeling, dataMaid, Hmisc, corrplot, minerva, caret, tidyverse)
```

#2. load data
The data set is from the funModeling package. The target variable is 'has_heart_disease' (0=bad consumer, 1=good consumer)
```{r}
data(heart_disease)
```


#3. EDA
First, let's look at the summary statistics and histograms of all the variables.
```{r}
html(describe(heart_disease))
#Alternative quick looks at the whole data set
#glimpse(heart_disease)
#summary(heart_disease)
```
The target variable is binary (has_heart_disease); we have 303 observations across 16 columns.
* Age is well distributed across the sample, near normal looking
* gender is binary
* chest_pain looks like a ranking from 1-4 on severity. Although they are buckets, we'll treat it as a numeric variable because it is ordered.
* resting blood pressure (numeric) has an interesting distribution. It may have outliers and may best be treated with binning of the values.
* serum cholesterol looks to have an outlier or a few that may need to be examined more closely. Otherwise the distribution looks fairly normal.
* fasting blood sugar is a binary response. Perhaps it is representitve of some cutoff for a reasonable tolerance or amount.
* Resting electro is 3 discrete categories; unknown if they have an order to them; unknown if should be treated as categorical or numeric. Will treat as numeric for now.
* max heart rate has a fairly normal distribution with potentially outliers on both ends that will need to be examined.
* exer angina is a binary response
* oldpeak--unsure what this variable represents. It looks like a very skewed distribution with a long right hand tail. We  many find that binning this variable results in better handling.
* slope: unsure what this variable means. there are 3 distinct categories, but not sure if they are ordered are not. the default is treating them as numeric, so we will continue as such. the first 2 values are well represented and the third is not.
*num vessels flour: have no idea what this variable means, but there are 4 missing values. It does appear to only have 4 responses. unsure if they are ordered responses, but since they are being treated as numeric we will treat them as such.
* thal is missing only 2 values. the categories assigned are 3, 6, and 7. We will treat this one as categorical because there are no inbetween values that show order. But perhaps this is a mistake and it should be treated numerically.
* heart disease severity is ordered. It is redundant with our target, 'has heart disease' as 0 is no heart disease and 1-4 is some expresion of heart disease. We will probably drop this variable, but will check in our EDA if we should keep it.
*exter angina: categorical response. unknown what exter angina is, but probably associated with exer angina.
*has heart disease: binary target, no missing values. 0=no hd 1=has hd


df_status is an easy way to look for missing values and correct varibale type assignment.
```{r}
df_status(heart_disease) 
```
fasting_blood_sugar has a high probability of a 0 value, so it may skew the results or not be a helpful predictor. We'll keep this in mind in our data exploration.
num_vessels_flour and thal are the only two variables with missing data (total of 6 records). Becasue there are so few, we might be okay in deleting these records, but let's remember to examine them.

```{r}
heart_disease %>%
  filter(is.na(num_vessels_flour) | is.na(thal))
```




This shows the the plot and table for the factor variables
```{r}
freq(heart_disease)
```
Chest pain should be treated as a numeric value because it has order in the values. The others are okay as factors. There are few 1's for chest pain, few 1's for fasting blood sugar, few 1's for resting electro, and few 6 for thal. We should keep these small values in mind as we fit our models and the important values are chosen. The values may result in over fitting.


Information for continuous variables
```{r}
profiling_num(heart_disease)
```

plots for continuous variables:
age, resting_blood_pressure, serum_cholestoral, max_heart_rate, exer_angina, oldpeak, slope, num_vessels_flour, heart_disease_severity
```{r}
heart_disease %>%
  select(age) %>%
  ggplot(aes(x=age)) +
  geom_histogram(binwidth = 3) +
  theme_minimal()

heart_disease %>%
  select(resting_blood_pressure) %>%
  ggplot(aes(x=resting_blood_pressure)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(serum_cholestoral) %>%
  ggplot(aes(x=serum_cholestoral)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(max_heart_rate) %>%
  ggplot(aes(x=max_heart_rate)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(exer_angina) %>%
  ggplot(aes(x=exer_angina)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(oldpeak) %>%
  ggplot(aes(x=oldpeak)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(slope) %>%
  ggplot(aes(x=slope)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(num_vessels_flour) %>%
  ggplot(aes(x=num_vessels_flour)) +
  geom_histogram() +
  theme_minimal()

heart_disease %>%
  select(heart_disease_severity) %>%
  ggplot(aes(x=heart_disease_severity)) +
  geom_histogram() +
  theme_minimal()
```

With the continuous variables, we're looking for NA values, outliers, odd distributions, variables that should be categorical, etc. Oldpeak is the only one that looks like it may have outliers/need to be treated differently if it's an important variable.


heart_disease %>%
  mutate(chest_pain = as.numeric(chest_pain),
         exer_angina = as.factor(exer_angina)) %>%
  select(-heart_disease_severity)

#EDA Continued: Looking at the Relationship between the target variable and the features we have
Correct the data type assignements and remove redundant columns
```{r}
heart_disease_all <- heart_disease %>%
  mutate(chest_pain = as.numeric(chest_pain),
         exer_angina = as.factor(exer_angina)) %>%
  select(-heart_disease_severity)
```




#Caret Feature Plot
```{r}
# pair-wise plots of all 4 attributes, dots colored by class
featurePlot(x=heart_disease[,1:5], y=heart_disease[,16], plot="pairs", auto.key=list(columns=6))
featurePlot(x=heart_disease[,6:15], y=heart_disease[,16], plot="pairs", auto.key=list(columns=5))
```


#Caret Density Plots
```{r}
# density plots for each attribute by class value
featurePlot(x=heart_disease[,1:15], y=heart_disease[,16], plot="density", scales=list(x=list(relation="free"), y=list(relation="free")), auto.key=list(columns=6))
```



#Categorical Variables
gender, fasting_blood_sugar, resting_electro, exer_angina, thal, exter_angina, target=has_heart_disease

```{r}
#blue is no, red is yes
cross_plot(data=heart_disease_all, input=c("gender", "fasting_blood_sugar", "resting_electro", "exer_angina", "thal", "exter_angina"), target="has_heart_disease", plot_type='percentual')
#gender: many nore men get heart disease (at least a higher percentage of men) compared to women. This looks like an important varibale in predicting if a patient will ahve heart disease
#fasting_blood_sugar: there does not appear to be much of a difference in this group between having heart disease and not having heart disease. We will probably be able to drop it, unless it interacts with another variable.
#resting_electro: there does appear to be a large difference between the three groups in their instance of getting heart disease.
#exer_angina: there appears to be a large differences in the instances of heart disease for this group.
#thal: group 3 is very differenct from 6 and 7, but 6 and 7 aren't quite as different. note the NA's
#exter_angina: significant difference.
```


Here, we need to make a decision about the NA values. How are we going to deal with them? What do they make sense as in this data set? Can we easily replace them with other methods? It looks like we probably just want to drop them as they can't be easily replaced and we won't loose much information without them.




#Continuous Variables (boxplots): 
age, chest_pain, resting_blood_pressure, serum_cholestoral, max_heart_rate, oldpeak, slope, num_vessels_flour
As a general rule, a variable will rank as more important if boxplots are not aligned horizontally.
Statistical tests: percentiles are another used feature used by them in order to determine -for example- if means across groups are or not the same.

```{r}
plotar(data=heart_disease_all, input=c("age", "chest_pain", "resting_blood_pressure", "serum_cholestoral", "max_heart_rate", "oldpeak", "slope", "num_vessels_flour"), target="has_heart_disease", plot_type = "boxplot")
#oldpeak looks useful; max_heart_rate looks useful; serum_cholesterol is very close; resting_blood pressure is very close; age looks good.
#age, chest_pain, max_heart_rate, old_peak, and num_vessels_flour all look like they have a potential to be important variables: there is a visually significant difference between the means and spread of the variables. One thing we should keep in mind though, is that it does look like we may have outliers that would impact regression models (but would be okay in random forest or gbm's because they are more robust models)
```



#Correlation and Relationships
```{r}
correlation_table(data=heart_disease_all, target="has_heart_disease") #retrieves R metric for all numeric variables skipping the categorical/nominal ones. Squaring this number returns the R-squared statistic (aka R2), which goes from 0 no correlation to 1 high correlation. the R statistic is highly influenced by outliers and non-linear relationships.
```



Look for correlation between the numeric features. We want to remove any varibales that are highly correlated with each other.
```{r}
#Check for correlation between numeric values
heart_disease_numeric <- heart_disease %>%
  select(age, chest_pain, resting_blood_pressure, serum_cholestoral, max_heart_rate, oldpeak, slope, num_vessels_flour) %>%
  mutate(chest_pain = as.numeric(chest_pain))

# calculate correlation matrix between numeric values. Remove one of a piar that is highly correlated as to remove duplicates
correlationMatrix <- cor(heart_disease_numeric)
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.7)
# print indexes of highly correlated attributes
print(highlyCorrelated)

#we are okay in that the numeric attributes are not correlated to each other. Yay!
```

Look for issues in correlation between the categorical features. We'll use MIC as our measure.
```{r}
#library(minerva)
#We need to do a data preparation trick, converting every categorical variable into flag (or dummy variable).
# selecting just a few variables
heart_disease_2 = 
  select(heart_disease, slope, num_vessels_flour, thal, heart_disease_severity, exter_angina, exer_angina, resting_electro, fasting_blood_sugar, chest_pain, gender, has_heart_disease)

# this conversion from categorical to a numeric is merely 
# to have a cleaner plot
heart_disease_2$has_heart_disease=
  ifelse(heart_disease_2$has_heart_disease=="yes", 1, 0)

# it converts all categorical variables (factor and 
# character for R) into numerical variables.
# skipping the original so the data is ready to use
dmy = dummyVars(" ~ .", data = heart_disease_2)

heart_disease_3 = data.frame(predict(dmy, newdata = heart_disease_2))

# Important: If you recieve this message 
# `Error: Missing values present in input variable 'x'. 
# Consider using use = 'pairwise.complete.obs'.` 
# is because data has missing values.
# Please don't omit NA without an impact analysis first, 
# in this case it is not important. 
heart_disease_4=na.omit(heart_disease_3)
  
# compute the mic!
mine_res_hd=mine(heart_disease_4)
#mine_res_hd
```

```{r}
#library(corrplot) 
diag(mine_res_hd$MIC)=0

# Correlation plot with circles. 
corrplot(mine_res_hd$MIC, 
         method="circle",
         # only display upper diagonal
         type="lower", 
         #label color, size and rotation
         tl.col="red",
         tl.cex = 0.9, 
         tl.srt=90, 
         # dont print diagonal (var against itself)
         diag=FALSE, 
         # accept a any matrix, mic in this case 
         #(not a correlation element)
         is.corr = F 
        
)
```

It looks like the only varibales with issues are within themselves (ie the flag's within thal are correlated, which is okay)
The below plot just adds the values to it., that's fine.

```{r}
# Correlation plot with color and correlation MIC
corrplot(mine_res_hd$MIC, 
         method="color",
         type="lower", 
         number.cex=0.7,
         # Add coefficient of correlation
         addCoef.col = "black", 
         tl.col="red", 
         tl.srt=90, 
         tl.cex = 0.9,
         diag=FALSE, 
         is.corr = F 
)
```
They are useful only when the number of variables are not big. Or if you perform a variable selection first, keeping in mind that every variable should be numerical.

If there is some categorical variable in the selection you can convert it into numerical first and inspect the relationship between the variables, thus sneak peak how certain values in categorical variables are more related to certain outcomes, like in this case.

Further than correlation, MIC or other information metric measure if there is a functional relationship.

A high MIC value indicates that the relationship between the two variables can be explained by a function. Is our job to find that function or predictive model.

#Variable Importance with MIC
```{r}
# Getting the index of the variable to 
# predict: has_heart_disease
target="has_heart_disease"
index_target=grep(target, colnames(heart_disease_4))

# master takes the index column number to calculate all
# the correlations
mic_predictive=mine(heart_disease_4, 
                    master = index_target)$MIC

# creating the data frame containing the results, 
# ordering descently by its correlation and excluding
# the correlation of target vs itself
df_predictive = 
  data.frame(variable=rownames(mic_predictive), 
                         mic=mic_predictive[,1], 
                         stringsAsFactors = F) %>% 
  arrange(-mic) %>% 
  filter(variable!=target)

# creating a colorful plot showing importance variable
# based on MIC measure
ggplot(df_predictive, 
       aes(x=reorder(variable, mic),y=mic, fill=variable)
       ) + 
  geom_bar(stat='identity') + 
  coord_flip() + 
  theme_bw() + 
  xlab("") + 
  ylab("Variable Importance (based on MIC)") + 
  guides(fill=FALSE)

#we know heart_disease_severity will be dropped, so we could rerun this plot to really see the importance of the other varibales.
```



Need a decision--can we drop these variables without making a large impact? They only make up 6 records of the whole data set, so it is within reason to remove them. Alternative methods would be creating flags for the missing data and filling in or imputing the varibales with a few different methods (ex missForest). We would typically make this decision at the end of this section/EDA.
```{r}
heart_disease_clean <- na.omit(heart_disease)
```

Correcting variable types and removing duplicate column
```{r}
heart_disease_all <- na.omit(heart_disease) %>%
  mutate(chest_pain = as.numeric(chest_pain),
         exer_angina = as.factor(exer_angina)) %>%
  select(-heart_disease_severity)
```

Our data frame with the correct variable names and dropped na values is called heart_disease_all


#Train/Test
Create Training and Testing Sets with an 80% split.
```{r}
set.seed(123)
training.samples <- heart_disease_all$has_heart_disease %>% 
  createDataPartition(p = 0.75, list = FALSE) #from caret package; stratified random sample w/in response variable, ie it keeps the balance
train.data  <- heart_disease_all[training.samples, ]
test.data <- heart_disease_all[-training.samples, ]
```
Notes: no missing values in the response to account for. Assuming a fairly even split between credit and no credit. Let's start building a few models. We'll start with the simplest (logistic regression) then work out way up (LASSO, Ridge, Elastic, XGBoost, CART)
Training Dataset: The sample of data used to fit the model.
Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.
Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.
The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained(using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner). Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world.
There are multiple ways to do this, and is commonly known as Cross Validation. Basically you use your training set to generate multiple splits of the Train and Validation sets. Cross validation avoids over fitting and is getting more and more popular, with K-fold Cross Validation being the most popular method of cross validation

Another method for customizing the tuning process is to modify the algorithm that is used to select the “best” parameter values, given the performance numbers. By default, the train function chooses the model with the largest performance value
train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: best is chooses the largest/smallest value, oneSE attempts to capture the spirit of Breiman et al (1984) and tolerance selects the least complex model within some percent tolerance of the best value. See ?best for more details.


#Model Building

#Simple Logistic Regression
```{r}
#start with using all of the variables
set.seed(123)

glm_mod_1 = train(
  form = has_heart_disease ~ ., #predicting class on all variables available
  data = train.data, #data set to build the model on
  trControl = trainControl(method = "cv", number = 10), #perform 10 fold cross validation; method specifies how the sampling should be completed
  method = "glm", #glm model
  family = "binomial" #response type
)

glm_mod_1
summary(glm_mod_1)

#we get a warning for this model because there are too many predictors for the model to handle, which is resulting in overfitting and multicolinearity. We can.....narrow down the list and select only a handful of predictors or we can continue on and hope the other methods help to resolve this issue. Let's start with reducing the variables based on our EDA
```

```{r}
#select the variables that looked important through the EDA
#gender; resting_electro; exer_angina; thal; exter_angina; age, chest_pain, max_heart_rate, old_peak, and num_vessels_flour
set.seed(123)

glm_mod_2 = train(
  form = has_heart_disease ~ gender + resting_electro + exer_angina + thal + exter_angina +
    age + chest_pain + max_heart_rate + oldpeak + num_vessels_flour, 
  data = train.data, #data set to build the model on
  trControl = trainControl(method = "cv", number = 10), #perform 10 fold cross validation; method specifies how the sampling should be completed
  method = "glm", #glm model
  family = "binomial" #response type
)

glm_mod_2
summary(glm_mod_2)
#still too many variables
```

#glmStepAIC
Generalized Linear Model with Stepwise Feature Selection
```{r}
#GLM STEP IN CARET
#method: glmStepAIC
#tuning: none
set.seed(123)

glmSTEP_mod = train(
  form = has_heart_disease ~ gender + chest_pain + resting_blood_pressure + 
    resting_electro + max_heart_rate + exer_angina + oldpeak + 
    num_vessels_flour + thal, 
  data = train.data, #data set to build the model on
  trControl = trainControl(method = "cv", number = 10), #perform 10 fold cross validation; method specifies how the sampling should be completed
  method = "glmStepAIC", #glm model
  family = "binomial" #response type
)

glmSTEP_mod
summary(glmSTEP_mod)
```



```{r}
#select the variables from the model below; stepwise selection with AIC
set.seed(123)

glm_mod_3 = train(
  form = has_heart_disease ~ gender + chest_pain + resting_blood_pressure + 
    resting_electro + max_heart_rate + exer_angina + oldpeak + 
    num_vessels_flour + thal, 
  data = train.data, #data set to build the model on
  trControl = trainControl(method = "cv", number = 10), #perform 10 fold cross validation; method specifies how the sampling should be completed
  method = "glm", #glm model
  family = "binomial" #response type
)

glm_mod_3
summary(glm_mod_3)
#Great! No error. We'll use this version for comparison across the models. Gets the exact same results as the below model.
```




Stepwise variable selection using AIC
From stepwise variable selection method using AIC, the significant variables are:
```{r}
set.seed(123)

disease.glm0 <- glm(has_heart_disease ~ ., family = binomial, train.data)
disease.glm.step <- step(disease.glm0, direction = "backward")
summary(disease.glm.step)
```
#
So to summarize, the basic principles that guide the use of the AIC are:
Lower indicates a more parsimonious model, relative to a model fit with a higher AIC.
It is a relative measure of model parsimony, so it only has meaning if we compare the AIC for alternate hypotheses (= different models of the data).
We can compare non-nested models. For instance, we could compare a linear to a non-linear model.
The comparisons are only valid for models that are fit to the same response data (ie values of y).
Model selection conducted with the AIC will choose the same model as leave-one-out cross validation (where we leave out one data point and fit the model, then evaluate its fit to that point) for large sample sizes.
You shouldn’t compare too many models with the AIC. You will run into the same problems with multiple model comparison as you would with p-values, in that you might by chance find a model with the lowest AIC, that isn’t truly the most appropriate model.
When using the AIC you might end up with multiple models that perform similarly to each other. So you have similar evidence weights for different alternate hypotheses. In the example above m3 is actually about as good as m1.
You should correct for small sample sizes if you use the AIC with small sample sizes, by using the AICc statistic.

#Boosted Logistic Regression
```{r}
#tuning parameter available: nTter
set.seed(123)

logboost_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "LogitBoost", #boosted logistic model
  family = "binomial" 
)

logboost_mod_1
summary(logboost_mod_1)
#works okay. Let's tune the model (only option is nIter) to see if we can improve accuracy slightly. If not, we'll move on quickly. Looks like we maximize accuracy from 11-21 iterations, so probably not beneficial to move beyond that. (see decrease in accuracy at 31.)
#more iterations doesn't help, selects the final model from 11 iterations. Just using the default is acceptable in this case.
```


Ridge and lasso aren't in the handbook for *logistic* regression models



#Bayes. Genneralized Linear Model
method = 'bayesglm'
no tuning parameters
```{r}
set.seed(123)

bayesglm_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, #data set to build the model on
  trControl = trainControl(method = "cv", number = 10), #perform 10 fold cross validation; method specifies how the sampling should be completed
  method = "bayesglm", #bayesian generalized linear model
  family = "binomial" #response type
)

bayesglm_mod_1
summary(bayesglm_mod_1)
#just as accurate as one of the above models
```


#Ordered Logistic or Probit Regression (response must have 3 or more levels!! Didn't run)
method = 'polr'


#Penalized Logistic Regression: doesn't work
  method = 'plr'
Tuning parameters:
lambda (L2 Penalty)
cp (Complexity Parameter)



#CART
model tuning: cp

Briefly, our goal here is to see if a smaller subtree can give us comparable results to the fully grown tree. If yes, we should go for the simpler tree because it reduces the likelihood of overfitting.
One possible robust strategy of pruning the tree (or stopping the tree to grow) consists of avoiding splitting a partition if the split does not significantly improves the overall quality of the model.
In rpart package, this is controlled by the complexity parameter (cp), which imposes a penalty to the tree for having two many splits. The default value is 0.01. The higher the cp, the smaller the tree.
A too small value of cp leads to overfitting and a too large cp value will result to a too small tree. Both cases decrease the predictive performance of the model.
An optimal cp value can be estimated by testing different cp values and using cross-validation approaches to determine the corresponding prediction accuracy of the model. The best cp is then defined as the one that maximize the cross-validation accuracy (Chapter @ref(cross-validation)).
Pruning can be easily performed in the caret package workflow, which invokes the rpart method for automatically testing different possible values of cp, then choose the optimal cp that maximize the cross-validation (“cv”) accuracy, and fit the final best CART model that explains the best our data.
*tuneLength, to specify the number of possible cp values to evaluate. Default value is 3, here we’ll use 10.
```{r}
set.seed(123)

rpart_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, #data set to build the model on
  trControl = trainControl(method = "cv", number = 10), #perform 10 fold cross validation; method specifies how the sampling should be completed
  method = "rpart", #logistic model trees
  tuneLength = 10
)

rpart_mod_1
plot(rpart_mod_1)
```

```{r}
#plot the tree
par(xpd = NA)
plot(rpart_mod_1$finalModel)
text(rpart_mod_1$finalModel,  digits = 3)
```


#Bagged CART
method = treebag
tuning: none
```{r}
set.seed(123)

brpart_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "treebag"
)

brpart_mod_1
```

#GBM (gradient Boosted Machine)
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.
```{r}
#method: gbm
#Stochastic Gradient Boosting Machines
#tuning: n.trees, interaction.depth, shrinkage, n.minobsinnode
set.seed(123)

gbm_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "gbm"
)

gbm_mod_1
summary(gbm_mod_1)
```

#SVM
```{r}
#method = svmLinearWeights2
#tuning: cost, Loss, weight
set.seed(123)

svm_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "svmLinearWeights2"
)

svm_mod_1
summary(svm_mod_1$finalModel)
```

#glmnet
This one is the most accurate. Cool!
```{r}
#method = glmnet
#tuning: alpha, lambda
set.seed(123)

glmnet_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "glmnet"
)

glmnet_mod_1
summary(glmnet_mod_1$finalModel)
```

#Linear Discriminant Analysis
Linear Discriminant Analysis with Stepwise Feature Selection; not as accurate.
There are a few other kinds of LDA we could try (regularized, penalized, robust, sparse) if wanted.
```{r}
#lda from the MASS package
#method = stepLDA
#tuning: maxvar, direction
set.seed(123)

lda_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "stepLDA"
)

lda_mod_1
summary(lda_mod_1$finalModel)
```

#simple neural network
```{r}
#method: nnet
#tuning: size, decay
set.seed(123)

nnet_mod_1 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "nnet"
)

nnet_mod_1
summary(nnet_mod_1$finalModel)
```

#Neural Networks with Feature Extraction
```{r}
#method: pcaNNet
#tuning: size, decay
set.seed(123)

nnet_mod_2 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "pcaNNet"
)

nnet_mod_2
summary(nnet_mod_2$finalModel)
#also not very accurate. both of the above could be tuned, but not needed
```


#random forest
can also try: oblique rf, or regularized rf
```{r}
#method = rf
#tuning= mtry
set.seed(123)

rf_mod_2 = train(
  form = has_heart_disease ~ ., 
  data = train.data, 
  trControl = trainControl(method = "cv", number = 10),
  method = "rf"
)

rf_mod_2
summary(rf_mod_2$finalModel)
#accuracy is similar to some of the earlier models, no problem. Worth comparing the testing results with as well.
```


glm X (have multiple glm (logistic regression) and a bayesian glm model)
rpart (have and have bagged rpart (CART method))
stochastic Gradient Boosting (method: gbm).
svm (need to make)
glmnet (method glmnet).
Linear discriminant analysis (method: lda).
Neural Network (method: nnet).
Random Forest (method : rf).





########
##Compare Models to Select a Winner
########
It is useful to get an idea of the spread of the models, perhaps one can be improved, or you can stop working on one that is clearly performing worse than the others.
Each model is automatically tuned and is evaluated using 3 repeats of 10-fold cross validation.
The random number seed is set before each algorithm is trained to ensure that each algorithm gets the same data partitions and repeats. This allows us to compare apples to apples in the final results.

I will use the same training/test split.
The learning problem(as an example) is the binary classification problem; predict customer churn. the data set can be found here.
Data pre-Processing has been performed using the recipe method.
A common setting/trainControl will be created, which will be shared by all considered classifiers(this will allow us to fit the models under the same conditions).
For tuning the hyperparameters the method adaptive_cv will be used (also allow us to reduce the tuning time).

Once the models are trained and an optimal parameter configuration found for each, the accuracy results from each of the best models are collected. Each “winning” model has 30 results (3 repeats of 10-fold cross validation). The objective of comparing results is to compare the accuracy distributions (30 values) between the models.

This is done in three ways. The distributions are summarized in terms of the percentiles. The distributions are summarized as box plots and finally the distributions are summarized as dot plots.

```{r}
#example comparison:
#If you needed to make strong claims about which algorithm was better, you could also use statistical hypothesis tests to statistically show that the differences in the results were significant. Something like a Student t-test if the results are normally distributed or a rank sum test if the distribution is unknown. In this post you discovered how you can use the caret R package to compare the results from multiple different models, even after their parameters have been optimized. You saw three ways the results can be compared, in table, box plot and a dot plot. The examples in this post are standalone and you can easily copy-and-paste them into your own project and adapt them for your problem.

# train the SVM model
set.seed(7)
modelSvm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# collect resamples
results <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
dotplot(results)
```

#For our top performing models, let's use the following to create a confusion matrix and get accuracy for the best models. We can pull the best models from the chart in the caretList file where all model accuraies are compared.
The most accurate models per that plot: svmLinearWeights2, glmnet, rf, gbm
These functions are wrappers for the specific prediction functions in each modeling package. In each case, the optimal tuning values given in the tuneValue slot of the finalModel object are used to predict.
```{r}
predict_svm <- predict.train(svm_mod_1, testing)
table(predict_svm)
confusionMatrix(predict_svm, testing[,"has_heart_disease"])

predict_glmnet <- predict.train(glmnet_mod_1, testing)
table(predict_glmnet)
confusionMatrix(predict_glmnet, testing[,"has_heart_disease"])

predict_rf <- predict.train(rf_mod_2, testing)
table(predict_rf)
confusionMatrix(predict_rf, testing[,"has_heart_disease"])

predict_gbm <- predict.train(gbm_mod_1, testing)
table(predict_gbm)
confusionMatrix(predict_gbm, testing[,"has_heart_disease"])

#gbm model has the highest accuracy on the test set. Se we will use it as our 'winner' and interpret it's coefficients and results
plot(gbm_mod_1$finalModel)
summary(gbm_mod_1$finalModel)
#relative.influence: At each split in each tree, gbm computes the improvement in the split-criterion (MSE for regression). gbm then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important.
#After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. For this we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves.

#PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. For example, consider the Gr_Liv_Area variable. The PDP plot below displays the average change in predicted sales price as we vary Gr_Liv_Area while holding all other variables constant. This is done by holding all variables constant for each observation in our training data set but then apply the unique values of Gr_Liv_Area for each observation. We then average the sale price across all the observations. This PDP illustrates how the predicted sales price increases as the square footage of the ground floor in a house increases.

#By definition, the closer the ROC curve comes to the upper left corner in the grid, the better it is. The upper left corner corresponds to a true positive rate of 1 and a false positive rate of 0. This further implies that good classifiers have bigger areas under the curves. As evident, our model has an AUC of 0.8389.

```

Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions. Whereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. This tutorial will cover the fundamentals of GBMs for regression problems.

Advantages:

Often provides predictive accuracy that cannot be beat.
Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.
No data pre-processing required - often works great with categorical and numerical values as is.
Handles missing data - imputation not required.
Disdvantages:

GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.
Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.
The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).












```{r}
#Model Evaluation
#In-sample misclassification rate
#Keeping cutoff probability as 0.1667, the misclassification rate is:

prob.glm1.insample <- predict(credit.glm.final, type = "response")
predicted.glm1.insample <- prob.glm1.insample > 0.1667
predicted.glm1.insample <- as.numeric(predicted.glm1.insample)
mean(ifelse(german_credit.train$response != predicted.glm1.insample, 1, 0))

#Confusion Matrix
#Checking for the predictions and seeing the False Positive and False negative values from the below confusion matrix:

table(german_credit.train$response, predicted.glm1.insample, dnn = c("Truth", "Predicted"))

#ROC Plot
#ROC Plot is plotted below and the AUC is 0.7896875

roc.plot(german_credit.train$response == "1", prob.glm1.insample)
roc.plot(german_credit.train$response == "1", prob.glm1.insample)$roc.vol$Area

#Out of sample misclassification rate and AUC score
#We get a misclassification rate of 0.395, and AUC of 0.7734524

prob.glm1.outsample <- predict(credit.glm.final, german_credit.test, type = "response")
predicted.glm1.outsample <- prob.glm1.outsample > 0.1667
predicted.glm1.outsample <- as.numeric(predicted.glm1.outsample)
table(german_credit.test$response, predicted.glm1.outsample, dnn = c("Truth", "Predicted"))
##      Predicted
## Truth  0  1
##     0 58 82
##     1  9 51
mean(ifelse(german_credit.test$response != predicted.glm1.outsample, 1, 0))
## [1] 0.455
roc.plot(german_credit.test$response == "1", prob.glm1.outsample)
roc.plot(german_credit.test$response == "1", prob.glm1.outsample)$roc.vol$Area

```


#Errors
RMSE for regression
  common to calculate in sample RMSE (too optomistic, leads to overfitting)
  better to calculate out of sample error (simulates real world use, helps avoid overfitting)







#Modeling NA's to replace them.
```{r}
# Excluding all NA rows from the data, in this case, NAs are not the main issue to solve, so we'll skip the 6 cases which have NA (or missing values).
heart_disease=na.omit(heart_disease)

# Setting a 4-fold cross-validation
fitControl = trainControl(method = "cv",
                           number = 4,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

# Creating the random forest model, finding the best tuning parameter set
set.seed(999)
fit_rf = train(x=select(heart_disease, -has_heart_disease, -heart_disease_severity),
             y = heart_disease$has_heart_disease,
             method = "rf",
             trControl = fitControl,
             verbose = FALSE,
             metric = "ROC")

# Creating the gradient boosting machine model, finding the best tuning parameter set
fit_gbm = train(x=select(heart_disease, -has_heart_disease, -heart_disease_severity),
             y = heart_disease$has_heart_disease,
             method = "gbm",
             trControl = fitControl,
             verbose = FALSE,
             metric = "ROC")




# Here we manipulate to show a nice the table described before
var_imp_rf=data.frame(varImp(fit_rf, scale=T)["importance"]) %>%
  dplyr::mutate(variable=rownames(.)) %>% dplyr::rename(importance_rf=Overall) %>%
  dplyr::arrange(-importance_rf) %>%
  dplyr::mutate(rank_rf=seq(1:nrow(.)))

var_imp_gbm=as.data.frame(varImp(fit_gbm, scale=T)["importance"])  %>%
  dplyr::mutate(variable=rownames(.)) %>% dplyr::rename(importance_gbm=Overall) %>%
  dplyr::arrange(-importance_gbm) %>%
  dplyr::mutate(rank_gbm=seq(1:nrow(.)))                                                                                                                            
final_res=merge(var_imp_rf, var_imp_gbm, by="variable")

final_res$rank_diff=final_res$rank_rf-final_res$rank_gbm

# Printing the results!
final_res


```




























#The importance of features can be estimated from data by building a model. Some methods like decision trees have a built in mechanism to report on variable importance. For other algorithms, the importance can be estimated using a ROC curve analysis conducted for each attribute. The most important have the highest value.
```{r}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Class~., data=GermanCredit, 
               method="lvq", 
               preProcess="scale", 
               trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

ggplot(model)

```






#Automatic feature selection methods can be used to build many models with different subsets of a dataset and identify those attributes that are and are not required to build an accurate model.

A popular automatic method for feature selection provided by the caret R package is called Recursive Feature Elimination or RFE.

The example below provides an example of the RFE method on the Pima Indians Diabetes dataset. A Random Forest algorithm is used on each iteration to evaluate the model. The algorithm is configured to explore all possible subsets of the attributes. All 8 attributes are selected in this example, although in the plot showing the accuracy of the different attribute subset sizes, we can see that just 4 attributes gives almost comparable results.



#APPENDIX
#Other Viz
```{r}
#continuous
GermanCredit %>%
  ggplot(aes(x= Class, y = Duration)) +
  geom_boxplot(aes(fill=as.factor(Class)))
#categorical
GermanCredit %>%
  ggplot(., aes(InstallmentRatePercentage, ..count..)) + 
  geom_bar(aes(fill = Class), position = "dodge") 
```

#Correlation
The following can be used fore feature selection: In this post you discovered 3 feature selection methods provided by the caret R package. Specifically, searching for and removing redundant features, ranking features by importance and automatically selecting a subset of the most predictive features.

#Generally, you want to remove attributes with an absolute correlation of 0.75 or higher BETWEEN EACH OTHER.
```{r}
# calculate correlation matrix
correlationMatrix <- cor(GermanCredit[,1:8])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
```



